{"/AI_trends/ChatGPT_SEO":{"title":"SEO를 위한 ChatGPT 3가지 사용 사례","data":{"":"디지털 마케팅, 특히 SEO 작업은 ChatGPT를 활용한다면 기존보다 더 빠르고 효율적으로 SEO 작업을 할 수 있습니다. 아래는 SEO에 작업에 ChatGPT를 활용할 수 있는 작업 3가지입니다.","robotstxt-생성하기#robots.txt 생성하기":"ChatGPT를 사용해서 robots.txt를 작성할 수 있습니다. 대부분의 웹사이트는 기본적으로 가입, 로그인, 로그아웃, 비밀번호 변경 페이지가 크롤링이 되지 않도록 robots.txt를 작성해야 합니다.","프롬프트#프롬프트":"register, login, logout, password 페이지가 크롤링되지 않도록 차단하는 robots.txt를 만듭니다.","chatgpt-응답#ChatGPT 응답":"User-agent: *\nDisallow: /register\nDisallow: /login\nDisallow: /logout\nDisallow: /password\n특정 크롤러가 특정 디렉터리에 액세스 하지 못하도록 방지하고 싶을 수 도 있습니다. 이럴 때는 이러한 규칙을 가진 robots.txt를 작성해 달라고 ChatGPT에게 부탁할 수 있습니다. 아래는 robots.txt 작업을 요청하는 프롬프트입니다.","프롬프트-1#프롬프트":"Bing이 /assets/ 디렉터리를 크롤링하는 것을 차단하지만 /assets/javascript/ 디렉터리를 크롤링하는 것을 허용하는 robots.txt 파일을 만듭니다.","chatgpt-응답-1#ChatGPT 응답":"User-agent: Bingbot\nDisallow: /assets/\nAllow: /assets/javascript/","메타-태그-생성하기#메타 태그 생성하기":"메타 태그는 웹페이지의 정보를 명시하기 위한 목적으로 사용하는 HTML 태그입니다.\n메타 태그는 <meta> 태그를 사용하며 HTML 문서에서 <head> 태그 아래 배치합니다. 메타 태그를 잘 작성한다면 SEO에 좀 더 유리해질 수 있습니다.\n사실, 메타 태그 작성을 도와주는 도구는 이미 마켓에 많이 나와있습니다. SEO 메타 태그 생성기를 활용할 수 도 있지만, ChatGPT를 사용하는 것도 가능합니다.","프롬프트-2#프롬프트":"페이지의 색인을 생성하고 링크를 따라갈 수 있지만 텍스트 스니펫의 문자 수를 46자로 제한하는 로봇의 메타 태그를 만듭니다.","chatgpt-응답-2#ChatGPT 응답":"<meta name=\"robots\" content=\"index, follow\">\n<meta name=\"googlebot\" content=\"index, follow\">\n<meta name=\"bingbot\" content=\"index, follow\">\n<meta name=\"description\" content=\"max-snippet:-1, max-image-preview:large, max-video-preview:-1\">","리다이렉션을-위한-htaccess-재작성-규칙-생성하기#리다이렉션을 위한 .htaccess 재작성 규칙 생성하기":"아파치 웹 서버에서 .htaccess 파일을 사용하고 있다면, .htaccess 작성을 할 때 ChatGPT의 도움을 받을 수 있습니다. .htaccess는 대소문자를 구분하고, 오류가 있을 경우에는 사이트 전체에 영향을 줍니다. .htaccess 작성은 이렇듯 상당히 까다로운 작업이기 때문에 ChatGPT에게 작성을 요청하는 것이 더 좋을 수 도 있습니다.","프롬프트-3#프롬프트":"301 리디렉션을 사용하여 /about.html을 /about-us/로 리디렉션하는 htaccess 재작성 규칙을 만듭니다.","chatgpt-응답-3#ChatGPT 응답":"RewriteEngine On\nRewriteRule ^about\\.html$ /about-us/ [R=301,L]\nChatGPT를 사용해서 SEO 작업을 하는 방법에 대해서 알아보았습니다. 이 외에도 여러 가지 작업에 ChatGPT의 도움을 받을 수 있기 때문에, SEO 작업은 이전보다 더 빠르게 진행할 수 있습니다. 그러나 한 가지 주의할 점은 ChatGPT의 출력이 맞는지 확인하는 과정이 반드시 필요하다는 점입니다.","참고#참고":"5 powerful chatgpt prompts for technical seo"}},"/AI_trends/google_io_2023":{"title":"Google I/O 2023에서 발표된 중요한 AI 소식 4가지","data":{"":"최근 마이크로소프트와 같은 경쟁업체들이 제네레이티브 AI 분야에서 주도권을 잡으면서 구글의 제너레이티브 AI는 상대적으로 뒤처지고 있다는 지적이 있었습니다.\n구글에서 개발한 바드는 잘못된 정보를 제시해서 알파벳의 주가가 7% 이상 폭락하기도 했습니다. 그런데 Google I/O 2023에서 발표된 AI와 관련된 기능들은\n이러한 실망감을 반전시킬 수 있겠다는 평가를 받았습니다. 이 글에서는 Google I/O 2023에서 발표된 AI 관련 기능들에 대해서 간단하게 알아보겠습니다.","1-palm2와-gemini#1. PaLM2와 Gemini":"구글이 공개한 PaLM2는 2022년 8월에 공개됐던 PaLM 모델보다 더 강력한 기능을 탑재하고 있으며, 가볍고 배포가 쉽습니다.\nPaLM2는 특히 도메인별 지식을 미세 조정하면 더욱 강력해진다는 특징이 있습니다. 현재 도메인별로 미세 조정된 사용 사례는 Sec-PaLM과 Med-PaLM 2가지가 있습니다.Sec-PaLM은 구글에서 최근 보안 사용 사례에 맞게 미세 조정한 모델입니다. 이 모델은 AI를 사용해 악성 스크립트를 더 잘 탐지합니다.Med-PaLM2는  의학적 지식으로 미세 조정한 모델입니다. 이 모델은 기존 모델에 비해 부정확한 추론을 9배 감소시켰으며,\n의학적 질문에 대해 임상 전문자의 수준에 근접한 답변을 제공할 수 있습니다. 또한, Med-PaLM2는 의료 영상에서 정보를 합성하는 기능도 제공합니다.\n예를 들어, 방사선 전문의가 이미지를 해석하고 결과를 전달하는 데 도움을 줄 수 있습니다.https://blog.google/technology/ai/google-palm-2-ai-large-language-model/Gemini는 구글 딥마인드에서 개발 중인 차세대 모델입니다. Gemeni는 아직 초기 단계이지만, 이전 모델에서는 없었던 다중 모드 기능이 있습니다. Gemini도 미세 조정된다면 PaLM2처럼 다양한 크기와 기능으로 사용할 수 있습니다.https://lifestyle.livemint.com/smart-living/innovation/google-gemini-ai-tool-open-ai-chatgpt-111684143093728.html","2-search-generative-experiencesge#2. Search Generative Experience(SGE)":"Google SGE\n구글은 전 세계 모든 검색 쿼리의 약 90%를 처리하며 검색 엔진 시작에서 가장 큰 점유율을 차지하고 있습니다. Google I/O에서 공개한 새로운 검색은 AI 기능을 구글의 자사 검색 엔진에 통합한 것으로, Bing Chat에 대응하는 기능이라고 할 수 있겠습니다.SGE는 PaLM2를 포함한 다양한 LLM으로 구동되며, 특히 검색 결과에 제시된 정보를 입증하는 웹 검색 결과를 식별하는 등 검색에 특화된 모델을 사용합니다. 모델은 핵심 순위 시스템과 함께 사용되어 신뢰할 수 있는 결과를 제공하도록 되어있습니다. 이러한 시스템으로 SGE를 제한해서 LLM의 한계인 할루시네이션과 부정확성을 완화합니다.SGE는 구글의 Search Labs에서 테스트해 볼 수 있습니다.","3-바드와-ai로-업그레이드한-구글-워크플레이스#3. 바드와 AI로 업그레이드한 구글 워크플레이스":"바드는 최근 PaLM2 모델로 옮겨져 수학, 추론, 코딩 및 많은 기능을 제공할 수 있게 되었습니다. 바드는 현재 일본어와 한국어로 제공되며,\n곧 40개 언어를 지원할 계획입니다. 바드는 구글 렌즈, 문서, 드라이브, Gmail, 지도 등 다양한 구글 앱 및 서비스 기능에 통합해 다양한 기능을 제공할 예정이며,\n사용자는 이러한 도구와 확장 프로그램을 어떻게 사용할지 결정할 때 개인 정보 설정을 제어할 수 있습니다.https://blog.google/technology/ai/google-bard-updates-io-2023/","4-구글-클라우드의-duet-ai#4. 구글 클라우드의 Duet AI":"Duet AI는 구글에서 클라우드 플랫폼에서 개발하는 개발자를 위한 코드 및 채팅을 지원하는 개발 인터페이스입니다.\nDuet AI는 실시간으로 입력되는 코드를 보고 코드를 추천해주고, 코드를 자동으로 완성하며, 코드의 취약성과 오류를 식별하는 동시에 코드 수정 사항도 제안합니다.https://cloud.google.com/blog/products/application-modernization/introducing-duet-ai-for-google-cloud?hl=enAppSheet는 제너테리티브 AI로 구동되는 노코드 서비스로, 간단한 프롬프트로 앱을 생성할 수 있습니다.\n이 글에서는 Google I/O 2023에서 발표한 주요 AI 소식 4가지에 대해서 살펴보았습니다.\nGoogle I/O 2023은 Microsoft, OpenAI와 같은 업체를 따라잡기 위해 AI 모델 개발 및 자사 제품과의 통합에 집중했다는 인상을 줍니다.Google I/O 2023에 대한 더 자세한 내용은 100 things we announced at I/O 2023 에서 확인할 수 있습니다.https://blog.google/technology/developers/google-io-2023-100-announcements/"}},"/AI_trends/pinecone/about_pinecone":{"title":"파인콘","data":{"":"최근 LLM이 중요한 이슈가 되면서, 많은 기업이 LLM의 장기 메모리 역할을 할 수 있는 벡터 데이터베이스에 관심을 보이고 있습니다.파인콘은 2001년 데이터 사이언티스트를 위한 벡터 데이터베이스를 출시한 스타트업입니다. 이 시리즈에서는 파인콘에서 제공하는 예제들을 살펴보면서 파인콘에 대해서 알아봅니다.","참고#참고":"Pinecone drops $100M investment on $750M valuation, as vector database demand grows"}},"/AI_trends/pinecone/create_table_qa_using_pinecone_and_langchain":{"title":"표 질의응답(Table Question Answering)을 파인콘(Pinecone), 랭체인(Langchain)으로 만들기","data":{"":"표 질의응답 모델(Table Question Answering)을 파인콘(Pinecone)과 랭체인(Langchain)을 사용해서 간단하게 만들어볼 수 있습니다. 이 글에서는 파인콘에서 제공하는 Colab을 참고해서 표 질의 응답을 만들어보겠습니다. 먼저, '표 질의응답 모델', '질의응답 모델'이 무엇인지 알아보도록 하겠습니다.","질의응답-모델#질의응답 모델":"질의응답(Question Answering) 모델이란, 문서 또는 자연어 텍스트를 기반으로 주어진 질문에 대한 대답을 생성하는 모델입니다. 질의응답을 하려면 모델은 텍스트를 이해하고 사실에 대한 추론 능력이 있어야 하기 때문에 다소 까다롭지만, 자주 묻는 질문에 대한 응답을 자동화할 수 있다는 편리함이 있어 이미 다양한 분야에서 많이 사용되고 있습니다. 질의응답 모델을 가장 접하기 쉬운 기능은 서비스의 고객 문의 챗봇이나 문서 검색입니다.","질의응답-모델의-종류#질의응답 모델의 종류":"질의응답은 입력 및 출력에 따라 다양한 종류가 있으며, 도메인을 기준으로 분류하기도 합니다.입력 및 출력에 따른 종류\nExtract QA: 모델이 콘텍스트로 제공된 텍스트 또는 HTML에서 답변을 추출합니다. 일반적으로 BERT 모델을 사용합니다.\nOpen Generative QA: 모델이 콘텍스트를 기반으로 직접 텍스트를 생성합니다.\nClosed Generative QA: 모델에게 콘텍스트를 제공하지 않고, 모델이 처음부터 끝까지 텍스트를 생성합니다.\n\n도메인에 따른 분류\n오픈 도메인(Open domain): 질문, 답변이 특정 도메인(예: 법률, 의료)으로 제한되지 않음\n클로즈 도메인(Closed domain): 질문, 답변이 특정 도메인으로 제한됨","표-질의응답-모델#표 질의응답 모델":"표 질의 응답(Table Question-Answering) 모델은 질의응답 모델과 마찬가지로 질문에 대한 답변을 생성하는 모델입니다. 모델에게 줄 콘텍스트는 표 형태로 제공해야 합니다. 아래는 표 질의응답 모델을 간략하게 설명하는 예시입니다.\n예를 들어, 아래와 같은 표를 모델에게 제공합니다.\n이름\t출연작\t이경영\t35\t오달수\t23\t조진웅\t19\n모델에게 '출연작이 가장 많은 배우가 누구인지 알려줘' 라고 질문한다면 모델은 출연작이 35편인 이경영을 답변으로 생성합니다.\n표 질의응답 모델은 다른 질의응답 모델과 자연어를 인식하고 처리하는 과정은 동일합니다. 그러나  표 질의응답 모델이 답변할 수 있는 내용은 주어진 표로 한정됩니다. 대체로 질문이 제한된 범위로 한정될 때 유용하게 사용할 수 있기 때문에, 고객 문의 챗봇에 많이 활용되고 있습니다.표 질의 응답 모델을 활용해서 시스템을 만들려면, 아래와 같은 세 가지 요소가 필요합니다.\n테이블 임베딩을 저장할 벡터 인텍스\n임베딩 쿼리와 테이블을 검색하는 검색(retriever) 모델\n테이블을 읽고 답변을 추출하는 리더(reader) 모델","파인콘-랭체인으로-표-질의응답-만들기#파인콘, 랭체인으로 표 질의응답 만들기":"이제 파인콘에서 제공하는 Colab을 참고해서 표 질의 응답을 만들어보겠습니다. Colab 사용이 처음이라면 Google Colab 질문과 답변을 참고합니다.","라이브러리-설치하기#라이브러리 설치하기":"!pip install datasets pinecone-client sentence_transformers torch-scatter\n아래는 각 라이브러리에 대한 설명입니다.\ndatasets: ML 모델을 학습하고 평가하는데 사용할 수 있는 공용 데이터 세트를 제공하는 라이브러리입니다.\npinecone-client: 파인콘 파이선 클라이언트입니다.\nsentence_transformers: 문장, 단락, 이미지에 대한 고밀더 벡터 표현을 쉽게 계산하는 방법을 제공하며 BERT/RoBERTa/XLM-RoBERTa등과 같은 트랜스포머를 기반으로 하는 프레임워크입니다.\ntorch-scatter: 파이토치에서 사용하기에 최적화된 sparse update(scatter and segment) 연산의 확장 라이브러리들로 구성되어 있습니다.","데이터-세트-로드하기#데이터 세트 로드하기":"위키피디아의 텍스트와 표로 구성된 OTT-QA (Open Table-and-Text Question Answering) 데이터 세트의 하위 세트로 작업할 것입니다. 하위 세트에는 20,000개의 테이블이 있으며, 아래와 같이 Huggingface Datasets 허브에서 로드할 수 있습니다.\nfrom datasets import load_dataset\n\n# load the dataset from huggingface datasets hub\ndata = load_dataset(\"ashraq/ott-qa-20k\", split=\"train\")\ndata\n\nDataset({\nfeatures: ['url', 'title', 'header', 'data', 'section_title', 'section_text', 'uid', 'intro'],\nnum_rows: 20000\n})\n\ndata[2]\n\n\n{'url': 'https://en.wikipedia.org/wiki/1976_New_York_Mets_season',\n'title': '1976 New York Mets season',\n'header': ['Level', 'Team', 'League', 'Manager'],\n'data': [['AAA', 'Tidewater Tides', 'International League', 'Tom Burgess'],\n['AA', 'Jackson Mets', 'Texas League', 'John Antonelli'],\n['A', 'Lynchburg Mets', 'Carolina League', 'Jack Aker'],\n['A', 'Wausau Mets', 'Midwest League', 'Bill Monbouquette'],\n['Rookie', 'Marion Mets', 'Appalachian League', 'Al Jackson']],\n'section_title': 'Farm system',\n'section_text': 'See also : Minor League Baseball',\n'uid': '1976_New_York_Mets_season_7',\n'intro': 'The New York Mets season was the 15th regular season for the Mets, who played home games at Shea Stadium. Led by manager Joe Frazier, the team had an 86-76 record and finished in third place in the National League East.'}\n테이터 세트에는 서로 관련있는 텍스트와 표가 있습니다. 이 예제에서는 표만 사용할 것 이기 때문에, 표를 추출해서 pandas 데이터 프레임으로 변환합니다.\nimport pandas as pd\n\n# store all tables in the tables list\ntables = []\n# loop through the dataset and convert tabular data to pandas dataframes\nfor doc in data:\ntable = pd.DataFrame(doc[\"data\"], columns=doc[\"header\"])\ntables.append(table)\n\ntables[2]\n\nLevel\tTeam\tLeague\tManager\tAAA\tTidewater Tides\tInternational League\tTom Burgess\tAA\tJackson Mets\tTexas League\tJohn Antonelli\tA\tLynchburg Mets\tCarolina League\tJack Aker\tA\tWausau Mets\tMidwest League\tBill Monbouquette\tRookie\tMarion Mets\tAppalachian League\tAl Jackson","검색기retriever-초기화#검색기(Retriever) 초기화":"검색기는 자연어 쿼리와 표 형식 데이터를 임베딩/벡터로 변환합니다. 자연어 질문과, 질문에 대한 답변이 포함된 표가 벡터 공간 가까운 곳에 위치하도록 임베딩을 생성합니다.검색 작업을 위해서 표 형식 데이터를 포함하도록 훈련한 Sentence Transformer 모델을 사용할 것입니다. 아래와 같이 Huggingface Models 허브에서 모델을 로드합니다.\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\n# set device to GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# load the table embedding model from huggingface models hub\nretriever = SentenceTransformer(\"deepset/all-mpnet-base-v2-table\", device=device)\nretriever\n\n\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n(2): Normalize()\n)\n\n표를 특정 형식으로 변환하는 함수를 작성합니다.\ndef _preprocess_tables(tables: list):\nprocessed = []\n# loop through all tables\nfor table in tables:\n# convert the table to csv and\nprocessed_table = \"\\n\".join([table.to_csv(index=False)])\n# add the processed table to processed list\nprocessed.append(processed_table)\nreturn processed\n여기에는 표만 사용하고 있습니다. 그러나 표를 검색하는 동안, 검색기가 메타데이터를 고려하도록 하려면 표의 시작 부분에서 줄 바꿈 문자로 구분된 title, section_title 등과 같은 모든 메타데이터 문자열을 조인할 수 있습니다.서식이 지정된 표를 살펴봅니다.\n# format all the dataframes in the tables list\nprocessed_tables = _preprocess_tables(tables)\n# display the formatted table\nprocessed_tables[2]\n\n\n\n'Level,Team,League,Manager\\nAAA,Tidewater Tides,International League,Tom Burgess\\nAA,Jackson Mets,Texas League,John Antonelli\\nA,Lynchburg Mets,Carolina League,Jack Aker\\nA,Wausau Mets,Midwest League,Bill Monbouquette\\nRookie,Marion Mets,Appalachian League,Al Jackson\\n'\n임베딩 모델이 형식이 지정된 표를 이해하고 정확하게 임베딩을 생성하도록 훈련됩니다.","파인콘-인덱스-초기화#파인콘 인덱스 초기화":"여기서는 파인콘 벡터 데이터베이스를 벡터 인덱스로 사용합니다. 파인콘 인덱스는 자연어 쿼리(쿼리 벡터)를 사용해 검색할 수 있는 표의 벡터 표현을 저장합니다. 파인콘은 쿼리 벡터와 벡터 인덱스에 저장된 포함된 표 간의 유사성을 계산합니다.파인콘을 사용하려면 먼저 파인콘과 연결을 초기화해야 합니다. 파인콘에서 무료 API 키를 발급받아, 아래와 같이 초기화합니다.\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\napi_key=\"YOUR API KEY\",\nenvironment=\"YOUR_ENVIRONMENT\"\n)\n\n새로운 인덱스를 생성합니다. 콘텍스트 임베딩을 생성하는 데 사용하는 검색기가 768차원 벡터를 출력하기 때문에, 메트릭 유형을 '코사인'으로 지정하고 차원을 768로 지정합니다. 파인콘은 코사인 유사성을 사용해 쿼리와 테이블 임베딩 간의 유사성을 계산합니다.\n# you can choose any name for the index\nindex_name = \"table-qa\"\n\n# check if the table-qa index exists\nif index_name not in pinecone.list_indexes():\n# create the index if it does not exist\npinecone.create_index(\nindex_name,\ndimension=768,\nmetric=\"cosine\"\n)\n\n# connect to table-qa index we created\nindex = pinecone.Index(index_name)","임베딩-생성-및-업서트#임베딩 생성 및 업서트":"테이블 임베딩을 생성하고, 이를 파인콘 인덱스에 업로드해야 합니다. 아래 코드는 테이블 임베딩을 생성하고 파인콘 인덱스에 업로드합니다.\nfrom tqdm.auto import tqdm\n\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(processed_tables), batch_size)):\n# find end of batch\ni_end = min(i+batch_size, len(processed_tables))\n# extract batch\nbatch = processed_tables[i:i_end]\n# generate embeddings for batch\nemb = retriever.encode(batch).tolist()\n# create unique IDs ranging from zero to the total number of tables in the dataset\nids = [f\"{idx}\" for idx in range(i, i_end)]\n# add all to upsert list\nto_upsert = list(zip(ids, emb))\n# upsert/insert these records to pinecone\n_ = index.upsert(vectors=to_upsert)\n\n# check that we have all vectors in index\nindex.describe_index_stats()\n\n100%|██████████| 313/313 [09:12<00:00, 1.49s/it]\n\n{'dimension': 768,\n'index_fullness': 0.0,\n'namespaces': {'': {'vector_count': 20000}},\n'total_vector_count': 20000}\n\n이제 파인콘 인덱스를 쿼리 할 준비가 되었습니다. 쿼리와 관련된 표를 변환하는지 테스트해 보겠습니다.\nquery = \"which country has the highest GDP in 2020?\"\n# generate embedding for the query\nxq = retriever.encode([query]).tolist()\n# query pinecone index to find the table containing answer to the query\nresult = index.query(xq, top_k=1)\nresult\n\n\n{'matches': [{'id': '19931', 'score': 0.822087, 'values': []}], 'namespace': ''}\n파인콘 인덱스는 82.2% 신뢰도로 쿼리에 대한 답변을 포함하는 표를 반환했습니다. 이 표에 실제로 답이 포함되어 있는지 확인해 보겠습니다. 반환된 표를, 인덱스를 사용해서 관련 pandas 데이터 프레임을 가져옵니다.\nid = int(result[\"matches\"][0][\"id\"])\ntables[id].head()\n\nRank\tCountry\tGDP (PPP, Peak Year) millions of USD\tPeak Year\t1\tChina\t27,804,953\t2020\t2\tIndia\t11,321,280\t2020\t3\tRussia\t4,389,960\t2019\t4\tIndonesia\t3,778,134\t2020\t5\tBrazil\t3,596,841\t2020\n파인콘 인덱스에 의해 반환된 표는 실제로 쿼리에 대한 답변을 포함하고 있습니다. 이제 이 표를 읽고 정확한 답을 추출할 수 있는 모델을 만들어봅니다.","표-리더-초기화#표 리더 초기화":"표 QA 작업을 위해 미세 조정된 TAPAS 모델을 사용합니다. TAPAS는 위키피디아의 대규모 영어 데이터 모음에서 자체 감독 방식으로 사전 훈련된 BERT와 유사한 트랜스포머(Transformer) 모델입니다. 허깅페이스 모델 허브에서 질문 응답 파이프라인으로 모델과 토크나이저를 로드합니다.\nfrom transformers import pipeline, TapasTokenizer, TapasForQuestionAnswering\n\nmodel_name = \"google/tapas-base-finetuned-wtq\"\n# load the tokenizer and the model from huggingface model hub\ntokenizer = TapasTokenizer.from_pretrained(model_name)\nmodel = TapasForQuestionAnswering.from_pretrained(model_name, local_files_only=False)\n# load the model and tokenizer into a question-answering pipeline\npipe = pipeline(\"table-question-answering\",  model=model, tokenizer=tokenizer, device=device)\n\n답을 추출하기 위해 이전에 사용했던 질문을 응답 파이프라인에 파인콘 인덱스와 쿼리가 반환한 표에 실행해 봅니다.\npipe(table=tables[id], query=query)\n\n\n{'answer': 'China',\n'coordinates': [(0, 1)],\n'cells': ['China'],\n'aggregator': 'NONE'}\n\n모델이 질문에 정확하게 대답했습니다.","쿼리#쿼리":"먼저 쿼리를 처리하고 표에서 답변을 추출하는 두 가지 함수를 정의합니다.\ndef query_pinecone(query):\n# generate embedding for the query\nxq = retriever.encode([query]).tolist()\n# query pinecone index to find the table containing answer to the query\nresult = index.query(xq, top_k=1)\n# return the relevant table from the tables list\nreturn tables[int(result[\"matches\"][0][\"id\"])]\n\ndef get_answer_from_table(table, query):\n# run the table and query through the question-answering pipeline\nanswers = pipe(table=table, query=query)\nreturn answers\n\n\nquery = \"which car manufacturers produce cars with a top speed of above 180 kph?\"\ntable = query_pinecone(query)\ntable\n\n\nManufacturer\tModel\tEngine\tPower Output\tMax-Speed(kph)\tDry Weight (kg)\tFiat\t805-405\tFIAT 1979ccS6 supercharged\t130 bhp\t220\t680\tAlfa Romeo\tGPR (P1)\tAlfa Romeo 1990cc S6\t95 bhp\t180\t850\tDiatto\tTipo 20 S\tDiatto 1997cc S4\t75 bhp\t155\t700\tBugatti\tType 32\tBugatti 1991cc S8\t100 bhp\t190\t660\tVoisin\tC6 Laboratoire\tVoisin 1978cc S6\t90 bhp\t175\t710\tSunbeam\tSunbeam 1988cc S6\t108 bhp\t180\t675\t\tMercedes\tM7294\tMercedes 1990cc S4 superchared\t120 bhp\t180\t750\tBenz\tRH Tropfenwagen\tBenz 1998cc S6\t95 bhp\t185\t745\tMiller\t122\tMiller 1978cc S8\t120 bhp\t186\t850","참고#참고":"Hugginface queestion-answering\nHow to build an Open-Domain Question Answering System?\nPinecone Table Question Answering"}},"/Korea_IT_trend/2023":{"title":"2023년 주요 IT 사건","data":{"":""}},"/about":{"title":"About","data":{"":""}},"/datacenter/datacenter_basic":{"title":"데이터센터 알아보기","data":{"":"네이버 데이터센터 각, 네이버, 2022요즘은 데이터센터 또는 IDC라는 용어를 일상생활에서도 자주 접할 수 있게 되었습니다. 그러나 많은 사람들이 데이터센터의 역할과 구성에 대해 잘 알지 못하는 것 같습니다.\n따라서 이 글에서는 데이터센터의 개념과 구성, 표준, 인증 등 데이터센터에 대해 알아두면 좋을 가벼운 지식에 대해 설명하겠습니다.","데이터센터란#데이터센터란?":"데이터센터(Data center, 또는 Datacenter)는 서버, 네트워크, 스토리지 등 IT 서비스 제공에 필요한 장비를 통합 관리하는 시설을 말합니다. 데이터센터는 어떤 상황에서도 IT 장비들을 안정적으로 운영할 수 있어야 하므로, 고도화된 전원 관리 시스템, 백업 시스템, 보안 시스템, 공조 시스템과 같은 다양한 시스템을 갖추고 있습니다. 데이터센터는 화재, 홍수, 또는 폭우나 태풍과 같은 자연재해도 극복할 수 있어야 합니다. 그래서 대부분의 데이터센터들은 이러한 장애에 대응하는 장애 대응 시나리오를 갖고 있으며, 내부 시설들도 장애 극복할 수 있도록 구축되어 있습니다.\n아래는 위키피디아의 설명입니다.\n데이터센터(data center)는 서버 컴퓨터와 네트워크 회선 등을 제공하는 건물이나 시설을 말한다. 서버 호텔(server hotel)이라고도 부른다.\n데이터센터는 인터넷의 보급과 함께 폭발적으로 성장하기 시작했다. 인터넷 검색, 쇼핑, 게임, 교육 등 방대한 정보를 저장하고 웹 사이트에 표시하기 위해\n수천, 수만 대의 서버 컴퓨터가 필요하게 되자, 이 서버 컴퓨터를 한 장소에 모아 안정적으로 관리하기 위한 목적으로 인터넷 데이터센터를 건립하게 되었다.","국내외-데이터센터-현황#국내외 데이터센터 현황":"세계 인터넷 트래픽 증가 추세에 맞춰 세계의 데이터센터 수요도 최근 몇 년 사이에 급증하고 있습니다. 현재는 전 세계에 8,200개의 데이터센터가 있습니다. 그중 3분의 1이 미국, 버지니아주에 분포해 있습니다. 전 세계 인터넷 트래픽의 70%가 버지니아주에서 발생하기 때문입니다.\n미국에서는 에퀴닉스(Equinix), 디지털 리얼티(Digital Realty) 등 데이터센터 리츠의 수익률이 과거에 비해 높아졌습니다.","국내-데이터센터-현황#국내 데이터센터 현황":"한국은 전력의 품질이 높지만 전기료는 높지 않은 편이기 때문에, 국내 데이터센터도 증가 추세입니다. 한국 데이터센터연합회에 따르면 2021년 기준 전국의 데이터센터 수는 177개입니다.\n이 중 상업용 62개, 비상업용 115개입니다. 약 60% 이상의 데이터센터가 수도권에 집중되어 있습니다.\n\n\n국내 데이터센터 현황, 한국 데이터센터연합회, 2022과거 데이터센터 개발은 통신기업이 주도했습니다. KT, LG 유플러스, SK텔레콤의 자회사 SK브로드밴드 등의 이동통신 국내 3사가 약 30개의 데이터센터를 운영하고 있습니다.\n최근에는 데이터센터 시장이 더 성장하면서 다양한 투자가가 데이터센터 시장에 등장하고 있습니다.\n\n\n국내 데이터센터 사업자 동향, Colliers, 2023\n한국의 데이터센터의 정책/표준, 기술/표준, 산업동향에 관한 자세한 설명은 한국데이터센터연합회를 참고합니다.","데이터센터-주요-설비#데이터센터 주요 설비":"데이터센터를 구성하는 주요 설비는 상면, 기반 시설, IT 장비, 운영으로 총 4 가지가 있습니다. 아래는 각 요소에 대한 설명입니다.","상면#상면":"상면은 장비나 설비를 설치할 수 있는 공간입니다. 실제 IT 장비를 설치할 수 있는 공간은 화이트 스페이스(white space),\n작업 통로나 설비 설치 공간과 같이 필요한 공간이지만 IT 장비를 설치할 수 없는 공간은 그레이 스페이스(gray space)로 구분하기도 합니다.\n\n\n화이트 스페이스, FS Community, 2023\n\n\n그레이 스페이스, FS Community, 2023","기반-시설#기반 시설":"데이터센터 운영을 지원하기 위해 필요한 공간과 설비입니다.\n기반 시설은 전력 및 통신을 공급하는 전기실, UPS 및 배터리, 발전기 등과 기계실, 항온항습기, 냉동기 등의 기반 설비로 구성됩니다.","it-장비#IT 장비":"컴퓨팅 서비스를 제공하기 위해 필요한 랙, 케이블링, 서버, 스토리지, 관리 시스템 및 네트워크 장비 등이 포함됩니다.\n\n\n캐비넷, 42U Network & Server Cabinet with Cable Managers, 800mm Wide, FS Community, 2023\n랙 마운트 서버 FS6400, Synology, 2023\n\n\nUPS power module, Huawei, 2022","운영#운영":"데이터센터가 24/7 무중단 서비스를 하려면 운영/관리 인력이 필요합니다. 운영은 주로 종합상황실과 네트워크 및 보안관리팀 두 가지의 팀으로 구성됩니다.\n종합상황실은 데이터센터 전체를 운영하며, 네트워크 및 보안관리실은 네트워크와 보안 설비 지원 시스템을 담당합니다.","데이터센터-분류와-등급#데이터센터 분류와 등급":"데이터센터 표준은 미국의 두 단체인 TIA(Telecommunications Industry Association)와\nUptime Institute에서 제시합니다.","데이터센터-규모-분류#데이터센터 규모 분류":"데이터센터를 규모에 따라 소형, 중형, 대형, 거대(massive), 메가(mega)로 분류합니다. IDC는 갈수록 대형화되는 추세입니다.\n최근에는 자체 변전소가 있는 하이퍼 스케일(hyper scale) 데이터센터도 많이 구축되고 있습니다.\n규모\t랙수\t면적\t메가(Mega)\t9,100대 이상\t22,501m² 이상\t거대(Massive)\t3,001~9,000대\t7,501~22,500m²\t대형(Large)\t801~3,000대\t2,001~7,500m²\t중형(Medium)\t201~800대\t501~2,000m²\t소형(Small)\t11~200대\t26~500m²\n\nData Center Size and Density, Strategic Directions, 2014.9","데이터센터-등급tier#데이터센터 등급(Tier)":"TIA(Telecommunications Industry Association)는 TIA-942에서 데이터센터를 4가지 등급으로 분류합니다.\n등급을 나누는 기준은 데이터센터의 각 기반 시설의 이중화 수준입니다. 아래는 각 등급에 대한 설명입니다.티어 1\n리던던트(Redundant) 구성요소가 없음(N) : 99.671% 가용성\n계획/비계획적인 활동에 따른 장애 가능성 있음\n전력 공급과 냉각이 단일 경로화되어 있음\n예방 유지 보수를 수행하기 위해 셧다운해야 함\n연간 28.8시간의 다운타임\n\n티어 2\n리던던트 구성요소(제한적인 N+1) : 99.741% 가용성\n계획/비계획적인 활동에 따른 장애 가능성이 Tier1에 비하여 낮음\n전력과 냉각 설비는 N+1 리던던트 구성으로 되어 있으나 공급 경로는 단일 경로로 되어 있음\nRaised Floor, UPS 및 발전기 포함\n연간 22.0시간의 다운타임\n\n티어 3\nConcurrent 유지 보수 환경(N+1) : 99.982% 가용성\n전산 시스템에 대한 영향 없이 정기 예방 점검과 유지 보수와 같은 계획된 활동이 가능함(비계획 활동은 장애를 유발시킬 수 있음)\n전력과 냉각의 공급 경로의 복수 구성(하나는 액티브 경로), 리던던트 구성(N+1)\n연간 1.6시간의 다운타임\n\n티어 4\nFault Tolerant(2N+1) : 99.995% 가용성\n계획된 활동은 핵심 운영에 영향을 주지 않으며, 예상치 못한 최악의 사고가 발생해도 핵심 부하에 영향을 주지는 않음\n복수의 액티브 전력 및 냉각 경로\n연간 0.4시간의 다운타임\n\n티어 1에서 티어 4로 갈수록 굉장히 안정적이고 신뢰도도 훨씬 높지만 비용이 굉장히 많이 발생합니다. 티어 4는 완전 무중단 운영이 가능합니다.\n티어 3은 운영하면서 고장난 장비를 수리할 수는 있어 운영에 문제가 없는 수준입니다. 그래서 일반적으로 데이터센터를\n설계할 때는 티어 3을 기본으로, 전력인입과 주요한 장비들, UPS, 전력 이중화를 2N으로 구성하도록 설계합니다.","데이터센터-관련-지표와-인증#데이터센터 관련 지표와 인증":"데이터센터의 에너지 효율을 표현하기 위한 여러 가지 지표와 인증이 있습니다. 이 글에서는 대표적인 데이터센터 관련 지표인 PUE, DciE와 인증인 LEED Certified에 대해서 알아보겠습니다.","puepower-usage-effectiveness#PUE(Power Usage Effectiveness)":"PUE는 데이터센터에 인입되는 전력량을 그 안에 있는 데이터센터의 인프라가 동작하면서 사용하는 전력량으로 나눈 값입니다. PUE 값으로 데이터센터의 에너지 효율성을 파악할 수 있습니다.\n예를 들어 100W의 IT 장비를 동작시키는데 275W를 공급해야 한다면, PUE = 275/100 = 2.75가 됩니다.\nPUE는 IT 소비 전력 대비 전체 전력량의 비율로 표시되며, 1에 가까울수록 효율성이 좋은 것을 의미합니다. 일반적으로 1.3(우수)에서 3.0(나쁨)에 걸쳐 있고 평균은 2.5 정도입니다.\nPUE = 데이터센터 에너지 총 소비량 ÷ IT 장비 에너지 소비량\n= (IT 전력 + IT 동력 + IT 기타) ÷ IT 전력\n\nPUE 계산식","dciedata-center-infrastructure-efficiency#DCiE(Data Center Infrastructure Efficiency)":"DCiE는 PUE와 함께 데이터센터의 효율을 표시하는 또 다른 단위로, IT 장비에 공급되는 전력량을 전체 설비에 대한 공급 전력량 기준으로 백분율로 표현한 것입니다.\n다시 말해 데이터센터 전체 전력 중 IT 장비가 사용하는 전력량이 몇 퍼센트인지 나타내며 100%에 근접할수록 효율이 좋은 것을 의미합니다.\n일반적인 DCiE 값은 33%(나쁨)에서부터 77%(우수)에 걸쳐 있고 평균 DCiE는 40%입니다.","leed-certified#LEED Certified":"LEEDLEED(Leadership in Energy and Environmental Design)는 건물이나 설비에서 물 같은 자원이나 기타 에너지를 효율적으로 사용하고 있는지에 대해 판단하고,\n이산화탄소 배출 감소와 실내 환경 품질(공조, 조명 등), 친환경 자재 채용 의무와 같은 항목을 다루고 있습니다.\nLEED 인증은 등급제이며, 아래와 같이 4단계가 있습니다.\n표준(Certified):계량 전 점수 대비 40%~50%\n실버(Silver): 50%~60%\n골드(Gold): 60%~80%\n플래티넘(Platinum): 80% 이상","마무리#마무리":"데이터센터의 중요성은 데이터의 증가와 디지털화의 추세로 더욱 커지고 있으며, 현대 비즈니스와 기술의 중요한 요소로 자리매김하고 있습니다.\n그러므로 앞으로 데이터센터와 관련된 용어들을 일상생활에서 더욱더 자주 접하게 될 것입니다.\n이 글이 데이터센터의 개념과 구성, 역할에 대해 이해하는 데 도움이 되었길 바랍니다.","참고#참고":"아마존, 구글, 네이버까지 투자하는 데이터센터! 그 중 왜 데이터센터 리츠에 주목해야할까?\n네이버 데이터센터 각\nNorthern Virginia still tops global data center markets\nIDC\n데이터센터 계획의 모든 것! 데이터센터 바이블"}},"/datacenter/datacenter_it_equipment":{"title":"데이터센터 IT 장비 알아보기","data":{"":"","랙과-캐비넷#랙과 캐비넷":"랙과 캐비넷랙은 서버 및 IT 장비를 고정하기 위해 여러 전자 장비 모듈을 장착하도록 설계된 개방형 프레임입니다.\n캐비닛은 전자 장비를 위한 여러 모듈을 수용할 수 있다는 점에서 랙과 같습니다. 캐비닛과 랙의 주요 차이점은 랙은 개방형이고 측벽이 없는 반면\n캐비닛은 전면 및 후면 도어, 측면 패널 및 지붕을 포함하여 모든 면이 밀폐되어 있다는 점입니다. 캐비닛은 폐쇄형 프레임 랙이라고도 합니다.","전산기계실-또는-상면#전산기계실 또는 상면":"화이트 스페이스, FS Community, 2023\n\n\n그레이 스페이스, FS Community, 2023전산기계실은 상면이라고도 합니다. 영어로는 화이트 스페이스(white space), 데이터 홀(data hall), 콜로(colo)라는 이름을 갖고 있습니다.\n예전에는 이중 바닥재 위에 흰색 전도성 타일을 깔았기 떄문에 화이트 스페이스(white space)라고 불렀습니다.\ndata hall은 데이터가 모이는 곳이기 때문에, colo는 코로케이션 사업을 위한 곳이기 때문에 이른 이름이 생겼습니다.\n전산기계실 양 끝에는 통신과 전기가 이중화 되어 들어오는 샤프트가, 양 끝에는 서버의 온도를 내리기 위한 항온항습실이 위치하는게 일반적인 구성입니다.","컨테인먼트#컨테인먼트":"컨테인먼트, TWP, 2023랙과 랙 사이의 복도를 아일(Aisle)이라고 합니다. 랙과 랙 사이를 묶어주는 구조물은 컨테인먼트라고 합니다.","핫-아일hot-aisle콜드-아일cold-aisle#핫 아일(Hot Aisle)/콜드 아일(Cold Aisle)":"핫 아일/콜드 아일, Siemon, 2023컨테인먼트와 서버룸을 냉각하는 방식에 따라 핫 아일과 콜드 아일 시스템으로 분류할 수 있습니다.\n뜨거워진 공기는 핫 아일을 통해서 천장 속으로 빠져 나가서 다시 항온항습기로 들어갑니다. 항온항습기에서 공기는 다시 차가워집니다.\n차가운 공기가 모이는 아일은 콜드 아일(Cold aisle)입니다. 차가운 공기는 콜드 아일을 지나 랙을 통과해서 뜨거워지고 핫 아일에 모입니다.","upsuninterruptible-power-supply#UPS(Uninterruptible Power Supply)":"UPS power module, Huawei, 2022전력 이중화의 핵심 장비는 전산장비와 바로 연결되는 무정전전원장치인 UPS입니다. UPS는 대부분 2n 으로 많이 구성합니다.\n한전 전원이 끊기게 되면 비상발전기가 가동됩니다. 그러나 발전기가 정상 가동될 때 까지 약간의 시간이 소요되는데, 그 시간 동안은 UPS에서 전력을 공급받습니다."}},"/AI_trends/use_stable_diffusion_on_mac":{"title":"애플 실리콘에서 스테이블 디퓨전 사용하기","data":{"":"Théâtre D'opéra Spatial.스테이블 디퓨전(Stable Diffusion)은 스타트업 스태빌리티 AI(Stability AI)가 여러 학술 연구원 및 비영리 단체와 공동으로 개발해 2022년에 오픈소스로 공개한 텍스트-이미지 생성 모델입니다. 최근 다양한 텍스트-이미지 생성 모델이 시장에 있지만, 오픈소스로 공개되어있는 모델은 스테이블 디퓨전이 유일합니다. 코드가 공개되어있는 덕분에 전세계의 많은 곳에서 다양한 방향으로 스테이블 디퓨전 모델을 확장 개발하고 있습니다. 스테이블 디퓨전이 다른 텍스트-이미지 생성 모델보다 많이 활용되고 있는 또 다른 이유는 스테이블 디퓨전으로 생성한 이미지가 '스페이스 오페라 극장'이라는 이름으로 2022년 콜로라도 주립 박물관 디지털 아트 대회에 출픔됐고, 우승을 했기 때문입니다.아래는 위키피디아의 스테이블 디퓨전에 대한 설명입니다.\n스테이블 디퓨전(Stable Diffusion)은 2022년에 출시된 딥 러닝, 텍스트-이미지 모델이다. 텍스트 설명에 따라 상세한 이미지를 생성하는 데 주로 사용되지만 인페인팅, 아웃페인팅, 이미지 생성과 같은 다른 작업에도 적용할 수 있다. 스타트업 스태빌리티 AI(Stability AI)가 여러 학술 연구원 및 비영리 단체와 공동으로 개발했다.\n스태빌리티 AI는 스테이블 디퓨전을 이용해 드림 스튜디오(Dream Studio)라는 유료 이미지 생성 서비스를 출시했습니다. 드림 스튜디오는 클립드롭, 포토샵, 블렌더에 사용할 수 있는 플러그인을 제공합니다. 드림 스튜디오는 공개한지 30일 만에 가입자 수가 100만명을 돌파했습니다. 드림 스튜디오의 장점은 다른 이미지 생성 서비스와 비교했을 때 상대적으로 서비스가 직관적이여서 사용하기가 쉽다는 점입니다.스테이블 디퓨전은 아래와 같이 4 가지 방법으로 사용할 수 있습니다.\nStable Diffusion: 웹에서 스테이블 디퓨전을 테스트할 수 있습니다.\nStable Diffusion web UI: 스테이블 디퓨전을 사용할 수 있는 웹 기반의 유저 인터페이스입니다. 로컬에 스테이블 디퓨전을 설치해서 사용합니다.\nDiffusers: 허깅페이스의 새 디퓨전 모델용 프레임워크입니다 스테이블 디퓨전을 파인 튜닝을 쉽게 할 수 있습니다.\nDiffusionBee: 스테이블 디퓨전을 실행할 수 있는 맥용 앱입니다.\nControlNet: 허깅페이스에서 스테이블 디퓨전을 테스트할 수 있습니다.\n\n이 글에서는 맥에 스테이블 디퓨전 웹 UI를 설치해서 사용하는 방법에 대해서 알아보겠습니다.","로컬에-스테이블-디퓨전-설치하기#로컬에 스테이블 디퓨전 설치하기":"","스텝1-패키지-설치하기#스텝1: 패키지 설치하기":"brew로 패키지를 설치합니다.\nbrew install cmake protobuf rust python@3.10 git wget","스텝2-깃으로-소스-클론하기#스텝2: 깃으로 소스 클론하기":"깃을 사용해 소스를 로컬에 클론합니다.\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui","스텝3-사용할-모델-추가하기#스텝3: 사용할 모델 추가하기":"스테이블 디퓨전은 다양한 종류의 모델이 있습니다. 사용하고 싶은 모델을 아래 위치에 위치시킵니다.\ntable-diffusion-webui/models/Stable-diffusion\n허깅페이스에서 다운로드 가능한 스테이블 디퓨전 모델을 확인할 수 있습니다. 모델을 다운로드 받으려면 Files and versions 탭을 클릭하고, \".ckpt\" 또는 \".safetensors\" 확장자로 나열된 파일을 찾습니다. 파일 크기 오른쪽에 있는 아래쪽 화살표를 클릭하여 다운로드합니다.\n여러개의 스테이블 디퓨전 모델을 다운로드 받아서 사용할 수 있습니다.아래는 가장 많이 사용되고 있는 디퓨전 모델입니다.\nStable DIffusion 1.4 (sd-v1-4.ckpt)\nStable Diffusion 1.5 (v1-5-pruned-emaonly.ckpt)\nStable Diffusion 1.5 Inpainting (sd-v1-5-inpainting.ckpt)\n\n스테이블 디퓨전 2.0 및 2.1에는 모델과 구성 파일이 모두 필요하며 이미지를 생성할 때 이미지 너비 및 높이를 768 이상으로 설정해야 합니다.\nStable Diffusion 2.0 (768-v-ema.ckpt)\nStable Diffusion 2.1 (v2-1_768-ema-pruned.ckpt)\n\n이 글에서는 Stable Diffusion 1.5를 사용합니다.\n스테이블 디퓨전 모델 1, 2의 차이는 Stable Diffusion 1 vs 2 What you need to know를 참고합니다.","스텝4-스테이블-디퓨전-web-ui-실행하기#스텝4: 스테이블 디퓨전 web UI 실행하기":"아래 명령어로 webui.sh 파일을 실행합니다.\n./webui.sh\n파일을 실행했을 때, 아래와 같은 에러가 발생할 수 있습니다.\nERROR: python3-venv is not installed, aborting...\n파이선 가상환경이 활성화되지 않아서 발생하는 문제입니다. 아래 명령어로 가상환경을 활성화한 뒤 webui.sh 파일을 실행합니다.\nsource venv/bin/activate","스텝5-웹-화면-확인하기#스텝5: 웹 화면 확인하기":"웹 브라우저에서 아래 주소로 접속합니다.\nhttp://127.0.0.1:7860\n아래 화면이 나타나면 스테이블 디퓨전이 정상적으로 실행된 것입니다.\n\n\n\n스테이블 디퓨전 메인 화면","스테이블-디퓨전-web-ui-사용하기#스테이블 디퓨전 web UI 사용하기":"스테이블 디퓨전에서 제공하는 세팅값에 대해서 알아봅니다.","스테이블-디퓨전-체크-포인트#스테이블 디퓨전 체크 포인트":"사용하고 싶은 모델을 선택할 수 있습니다.","프롬프트-네거티브-프롬프트#프롬프트, 네거티브 프롬프트":"미드저니는 하나의 프롬프트만 입력합니다. 스테이블 디퓨전은 프롬프트, 네거티브 프롬프트(negative prompt) 이렇게 총 2 개의 프롬프트를 입력할 수 있습니다.\n아래는 각 프롬프트에 관한 설명입니다.\n프롬프트: 이미지에 반영되기를 원하는 내용을 지시하는 프롬프트.\n네거티브 프롬프트: 이미지에 반영되지 않기를 원하는 내용을 지시하는 프롬프트.","샘플링-메서드sampling-method#샘플링 메서드(Sampling method)":"샘플링 메서드는 샘플링하는 방법을 말합니다. 현재 가장 많이 사용되는 샘플링 메서는 아래와 같습니다.\nEuler a\nDPM++ 2M Karras\nDPM++ SDE Karras","샘플링-스텝sampling-steps#샘플링 스텝(Sampling steps)":"그림을 생성할 때 샘플링을 몇 단계를 거칠 것인지 결정합니다. 0~150까지 설정할 수 있으며, 숫자가 증가할 수록 더 많은 샘플링 단계를 거칩니다.","얼굴-보정restore-faces타일링tiling#얼굴 보정(Restore faces)/타일링(Tiling)":"Restore faces는 얼굴을 보정합니다. 타일링은 이미지를 타일 패턴으로 만듭니다.","hiresfix#Hires.fix":"Hires.fix는 이미지를 고해상도로 보정합니다.","cfg-scale#CFG Scale":"프롬프트의 영향력을 설정합니다. 수치가 낮을수록 프롬프트에 강하게 의존합니다. 수치가 높을수록 자유도가 증가합니다. 적정 수치는 7~9입니다.","배치-카운트batch-count배치-사이즈batch-size#배치 카운트(Batch count)/배치 사이즈(Batch size)":"한 번에 여러 장의 이미지를 그릴 때 사용합니다. 배치 카운트는 생성할 배치 수 입니다. 배치 사이즈는 하나의 배치에 들어갈 이미지의 수 입니다.","시드seed#시드(Seed)":"이미지의 고유 번호입니다. 동일한 프롬프트를 주더라도, 시드 값에 따라 다른 이미지가 생성됩니다. 시드의 기본값은 -1입니다. -1일 때는 시드가 랜덤으로 설정됩니다.","콘트롤넷contrlnet#콘트롤넷(ContrlNet)":"피사체의 자세, 구도를 설정합니다.","트러블슈팅#트러블슈팅":"트러블슈팅은 스테이블 디퓨전의 공식 문서 troubleshooting 부분을 참고합니다.","참고#참고":"Wikipedia-Stable Diffusion\nAn A.I.-Generated Picture Won an Art Prize. Artists Aren't happy."}},"/datacenter/datacenter_lifecycle":{"title":"데이터 센터 라이프 사이클","data":{"":"데이터 센터의 라이프 사이클은 계획, 설계, 구축, 신뢰성 테스트, 운영, 종료로 이뤄집니다. 아래는 각 라이프 사이클 단계에 대한 간략한 설명입니다.\n계획: 데이터 센터 요구사항 정의. 사업성평가 등 마스터플랜 수립\n설계: 개념설계, 실시설계, 인허가 등\n구축: 발주 및 구축(시공)\n신뢰성 테스트: 센터 내 각 시설장비의 환경, 조건별 작동 여부 점검 및 조정\n운영: 시스템 및 장비 이전 구축, 운영 및 최적화\n종료: 데이터 센터 운영 종료","계획#계획":"데이터 센터의 안정적 운영은 해당 기업의 업무 연속성과 같은 깊은 연관성을 가집니다. 그래서 도시 및 건축 계획적으로 규모 외에도 수해와 같은 자연재해 같은 또는 위험한 시설들이 주변에 있는지를 확인한 후에 부지를 선정합니다.\n부지가 이러한 위험들로부터 안전한 것으로 판단되면, 그다음은 인프라 공급 여부에 따라서 최종적으로 부지를 결정합니다.\n데이터 센터를 계획할때 가장 중요한 인프라 중 하나는 전력입니다. 데이터 센터의 전력은 서로 다른 두 변전소에서 다른 경로를 통해 전력을 a 변전소, b 변전소로부터 충분히 받을 수 있는지, 통신은 모든 사업자로부터 받을 수 있는지, 그리고 상하수도나 가스 등 인프라를 얼마나 공급받을 수 있는지 검토한 이후에 최종 선정합니다.\n운영 관점에서, 부지는 접근성이 용이한 곳이 좋습니다. 데이터센터에 장애가 발생하면 신속하세 수습하고 처리해야하기 때문입니다.","설계#설계":"데이터 센터 설계는 작은 서버인 유니트부터 시작합니다. 유니트는 폭이 약 4.5cm 정도 되는 표준 서버입니다.\n이 유니트를 여러개 꽂을 수 있는 장비를 랙(Rack)이라고 부릅니다. 이 랙을 2열로 배치하면 컨테인먼트(Containment)가 됩니다.\n이 컨테인먼트를 여러 배치하면 모듈(Module)이됩니다. 모듈은 전산기계실이라고도 부릅니다.\n이러한 모듈 또는 전산기게실은 건물의 각 층에 한 개 또는 그 이상을 배치할 수 있습니다.\n이렇게 여러개가 배치된 층을 플로어(Floor)라고 부릅니다. 전산기계실 기준층과 기반 시설이 모이면 한 개의 데이터 센터가 됩니다.\n아래는 데이터 센터를 설계할 때 고려하는 가장 작은 단위인 유니트부터 가장 큰 단위인 데이터 센터까지에 대한 설명입니다.\n유니트(Unit): 4.5cm 정도 폭인 1 개의 표준 서버\n랙(Rack): 유니트를 꽂는 장비\n컨테인먼트(Containment): 랙을 2열로 배치한 것\n모듈(Module): 컨테인먼트를 수형으로 여러개 배치한 것. 모듈이 전산기계실이 됨\n플로어(Floor): 각 층에는 전산기계실을 한 개 또는 그 이상으로 배치할 수 있음\n데이터 센터: 전산기계실 기준층과 기반 시설이 모여서 한 개의 데이터 센터가 됨","랙과-캐비넷#랙과 캐비넷":"랙과 캐비넷데이터 센터와 관련해서 캐비닛이라는 용어를 들어본 적이 있을 것입니다. 랙은 서버 및 IT 장비를 고정하기 위해 여러 전자 장비 모듈을 장착하도록 설계된 개방형 프레임입니다.\n캐비닛은 전자 장비를 위한 여러 모듈을 수용할 수 있다는 점에서 랙과 같습니다. 캐비닛과 랙의 주요 차이점은 랙은 개방형이고 측벽이 없는 반면\n캐비닛은 전면 및 후면 도어, 측면 패널 및 지붕을 포함하여 모든 면이 밀폐되어 있다는 점입니다. 캐비닛은 폐쇄형 프레임 랙이라고도 합니다.","전산기계실-또는-상면#전산기계실 또는 상면":"전산기계실은 상면이라고도 합니다. 영어로는 white space, data hall, colo라는 이름을 갖고 있습니다. 예전에는 이중 바닥재 위에 흰색 전도성 타일을 깔았기 떄문에 white space라고 불렀습니다. data hall은 데이터가 모이는 곳이기 때문에, colo는 코로케이션 사업을 위한 곳이기 때문에 이른 이름이 생겼습니다. 전산기계실 양 끝에는 통신과 전기가 이중화 되어 들어오는 샤프트가, 양 끝에는 서버의 온도를 내리기 위한 항온항습실이 위치하는게 일반적인 구성입니다.","핫-아일hot-aisle-콜드-아일cold-aisle#핫 아일(Hot Aisle), 콜드 아일(Cold Aisle)":"랙과 랙 사이의 복도를 아일(Aisle)이라고 합니다. 랙과 랙 사이를 묶어주는 구조물은 컨테인먼트라고 합니다. 냉각 효율을 높이기 위해서 랙은 전면끼리 또는 그 뒷면 끼리 마주보게 배치합니다. 컨테인먼트와 서버룸을 냉각하는 방식에 따라 핫 아일과 콜드 아일 시스템으로 분류할 수 있습니다.\n뜨거워진 공기는 핫 아일을 통해서 천장 속으로 빠져 나가서 다시 항온항습기로 들어갑니다. 항온항습기에서 공기는 다시 차가워집니다. 차가운 공기가 모이는 아일은 콜드 아일(Cold aisle)입니다. 차가운 공기는 콜드 아일을 지나 랙을 통과해서 뜨거워지고 핫 아일에 모입니다. 이 때 핫 아일과 콜드 아일을 구분하는 구조물이 컨테인먼트입니다. 컨테이먼트는 전산센터의 냉각 효율 높여줍니다.","upsuninterruptible-power-supply#UPS(Uninterruptible Power Supply)":"화웨이의 UPS power module전력 이중화의 핵심 장비는 전산장비와 바로 연결되는 UPS입니다.\nUPS는 무정전전원 장치라고 합니다. 대부분 2n 으로 많이 구성합니다. 한전 전원이 끊기게 되면 비상발전기가 가동됩니다. 그런데 발전기가 정상 가동될 때 까지 시간이 소요됩니다.\n이 시간 만큼을 벌어 주기 위한 장비가 배터리입니다.","구축#구축":"","운영#운영":"","신뢰성-테스트#신뢰성 테스트":"","종료#종료":"","참고#참고":""}},"/datacenter/datacenter_terminology":{"title":"데이터센터 용어집","data":{"":"","ㄷ#ㄷ":"디지털 리얼티(Digital realty): 데이터 센터, 코로케이션 및 상호 연결 전략을 전문으로 하는 미국의 다국적 기업.","ㄹ#ㄹ":"랙(Rack): 서버 및 IT 장비를 고정하기 위해 여러 전자 장비 모듈을 장착하도록 설계된 개방형 프레임.","ㅁ#ㅁ":"무정전전원장치(Uninterruptible Power Supply, UPS): 무정전 전원장치로 전원이 정전되었을 때 부하전력의 연속성 확보를 위해 사용하는 전원장치.","ㅅ#ㅅ":"상면: IT 장비나 설비를 설치할 수 있는 공간.","ㅇ#ㅇ":"에퀴닉스(Equinix): 데이터 센터를 전문으로 하는 미국의 다국적 기업.\nEquinix, Inc.는 캘리포니아 레드우드 시티 에 본사를 둔 미국의 다국적 기업으로 인터넷 연결 및 데이터 센터를 전문으로 합니다. 이 회사는 5개 대륙의 27개국에 248개의 데이터 센터를 보유한 글로벌 코로케이션 데이터 센터 시장 점유율 의 선두 주자입니다.\nNASDAQ 증권 거래소 에 EQIX라는 이름으로 상장되어 있으며 2022년 12월 현재 전 세계적으로 약 12,000명의 직원이 근무하고 있습니다. 2015년 1월 부동산투자신탁 (REIT) 으로 전환했습니다.","ㅋ#ㅋ":"캐비넷: 서버 및 IT 장비를 고정하기 위해 여러 전자 장비 모듈을 장착하도록 설계되어 랙과 비슷하나, 전면 및 후면 도어, 측면 패널 및 지붕을 포함하여 모든 면이 밀폐되어 있어 폐쇄형 프레임 랙이라고도 함.","ㅎ#ㅎ":"하이퍼 스케일(Hyper scale) 데이터센터: 서버 10만대 이상 수용 가능한 데이터센터."}},"/datacenter/hyperscale_datacenter":{"title":"하이퍼스케일 데이터센터 알아보기","data":{"":"각 세종, 네이버 클라우드, 2022시너지 리서치 그룹(Synergy Research Group)이 발표한\n하이퍼 스케일 데이터센터 현황에 따르면, 전 세계에서 운영 중인 하이퍼 스케일 데이터센터 수는 2021년 3분기 말 기준으로 700개에 달합니다.\n하이퍼스케일 데이터센터 수가 2배가 되는 데는 5년이 걸렸지만, 용량이 2배 되는 데는 4년이 채 걸리지 않은 것으로 나타났습니다. 이렇듯 하이퍼스케일 데이터센터의 수와 평균 크기는 매년 꾸준하게 증가하고 있으며,\n그 수요도 증가하고 있습니다. 이 글에서는 하이퍼스케일 데이터센터의 기준과 핵심 요소, 그리고 국내외 하이퍼스케일 데이터센터에 대해서 알아봅니다.","하이퍼스케일-데이터센터-규모#하이퍼스케일 데이터센터 규모":"하이퍼스케일의 공식적인 정의는 아직 없으나, 기업에서 하이퍼스케일 데이터센터에 요구하는 요구사항을 정리하면 아래와 같습니다.\n하이퍼스케일 데이터센터는 5,000대 이상의 서버를 갖추면서 10,000평방피트를 초과하는 규모여야합니다.\n네트워크 연결은 초당 40기가바이트(Gbps)이여야 합니다. PUE(전략 사용량 효율성)의 경우, 대부분의 엔터프라이즈 데이터센터는 일반적으로 1.67 ~ 1.8 사이의 PUE를 보여줍니다.\n반면, 구글의 하이퍼스케일 데이터센터는 PUE 값이 1.1입니다. PUE 1.0은 완벽한 효율성을 의미합니다.\n이렇게 기존 데이터센터의 규모보다 훨씬 거대한 규모, 모든 부하를 충족할 수 있도록 민첩하게 확장, 축소할 수 있는 역량, 완전에 가까운 자동화등의 역량을 갖춘 데이터센터를 하이퍼스케일 데이터센터라고 합니다.\n규모에 따른 데이터센터 분류는\n데이터센터 알아보기의 데이터센터 규모 분류를 참고합니다.","하이퍼스케일-데이터센터-핵심-요소-4가지#하이퍼스케일 데이터센터 핵심 요소 4가지":"하이퍼스케일 데이터센터는 아래와 같은 요소를 고려해서 설계 및 구축되어야 합니다.\n부지 입지: 부지 입지는 시설이 제공할 수 있는 서비스 품질(QoS)에 영향을 미치기 때문에 하이퍼스케일 데이터 센터를 설계할 때 기업이 고려해야 하는 가장 중요한 고려 사항 중 하나일 것입니다. 예를 들어 시골 지역에 시설을 배치하는 것이 비용이 덜 들 수 있지만 도시 지역의 최종 사용자까지의 거리로 인해 상당한 처리 지연이 발생할 수 있습니다. 또한 불규칙한 전력망을 사용하면 비용이 많이 드는 정전이 발생할 수 있습니다.\n전력 공급원: 하이퍼스케일 데이터 센터는 서버를 운영하고 시스템을 냉각할 때 엄청난 양의 전기를 소비합니다. 많은 하이퍼스케일 데이터 센터는 PUE(Power Usage Effectiveness)가 낮지만 크기와 전력 수요 때문에 전력이 저렴한 지역에 구축해야 합니다. 또는 회사는 효율성을 향상시키기 위해 태양광 및 풍력과 같은 지속 가능한 에너지원을 고려할 수 있습니다.\n자동화: 하이퍼스케일 데이터 센터에는 일반적으로 수천 대의 서버와 스위치, 라우터, 저장 장치를 비롯한 기타 하드웨어 구성 요소가 있습니다. 또한 전원 및 냉각 시스템, 공기 분배 시스템, 무정전 전원 공급 장치(UPS)와 같은 인프라 구성 요소도 수용합니다. 민첩성을 방해하고 효율성을 최소화하기 때문에 이러한 시스템을 대규모로 수동으로 모니터링하고 구성하는 것은 불가능합니다. 이러한 시설을 효율적으로 관리하기 위해 하이퍼스케일 데이터 센터는 일반적으로 자동화 및 오케스트레이션에 의존하여 스케줄링, 모니터링 및 워크로드 제공과 같은 워크플로우를 관리합니다. 예를 들어 기업은 자동화 도구를 활용하여 사용 가능한 전기 및 냉각을 기반으로 리소스를 할당할 수 있습니다.\n보안: 하이퍼스케일 데이터 센터의 가치와 조직에 대한 중요한 중요성을 고려할 때 시설과 IT 리소스를 보호해야 합니다. 이와 관련하여 하이퍼스케일 데이터 센터를 구성하는 구조의 설계는 액세스 누출 위험을 최소화해야 합니다. 예를 들어, 하이퍼스케일 데이터 센터를 구축할 때 경계 울타리, 재료의 두께, 벽을 만드는 데 사용되는 재료의 유형을 고려해야 합니다.1","세계에서-가장-거대한-5개의-하이퍼스케일-데이터센터#세계에서 가장 거대한 5개의 하이퍼스케일 데이터센터":"세계에서 가장 거대한 5개의 하이퍼스케일 데이터센터에 대해서 알아봅니다.\n내몽골 정보 허브(The Inner Mongolian Information Hub)\n\n\n총면적: 1070만 평방피트\nChina Telecom이 소유한 Inner Mongolian Information Hub는 후허하오터에 있는 6개의 데이터 센터 중 가장 큰 규모입니다.\n\n\n후허하오터 데이터 센터(Hohhot Data Center)\n\nHohhot Data Ceneter, xinhuanet, 2022\n\n\n총면적: 770만 평방피트\n차이나 모바일의 후허하오터 데이터 센터는 후허하오터 정보 허브에 위치한 두 번째로 큰 데이터 센터입니다.\n\n\n더 시타델 캠퍼스(The Citadel Campus)\n\nThe Citadel Campus, Data Center Knowledge, 2017\n\n\n총 면적: 720만 평방피트\n네바다주 북부에 위치한 더 시타델 캠퍼스는 글로벌 기술 회사인 스위치(Switch)가 소유한 세계 최대의 데이터 센터입니다. 최대 650메가와트(MW)의 100% 재생 가능 에너지로 구동됩니다. 캠퍼스는 실리콘 밸리에 4.5밀리초(ms), 실리콘 비치에 9ms, 라스베이거스에 7ms의 대기 시간을 보장합니다.\n\n\n레인지 인터네셔널 인포메이션 허브(Range International Information Hub), 랑팡(Langfang), 중국\n\n\n총면적: 660만 평방피트\n베이징과 톈진 사이의 중국 랑팡에 위치한 레인지 인터네셔널 인포메이션 허브는 IBM과 협력하여 설계된 4세대 슈퍼 데이터 센터입니다.\n\n\nSwitch SuperNAP, 네바다주 라스베이거스\n\nSwitch SuperNAP, Switch\n\n\n총면적: 330만 평방피트\nSuperNAP도 Switch가 소유하고 있으며 라스베가스에 있습니다. SuperNAP는 최대 531MW의 100% 친환경 에너지로 작동합니다.","국내-하이퍼스케일-데이터센터-동향#국내 하이퍼스케일 데이터센터 동향":"국내에서도 데이터센터 수요가 증가하면서 덩달아 하이퍼스케일 데이터센터도 증가하고 있습니다. 현재 한국에는 하이퍼스케일 기준을 충족하는 데이터센터는 두 곳이 있습니다.\n한군데는 평촌LG유플러스 데이터센터이고, 다른 하나는 네이버의 데이터센터 각 세종입니다.","평촌lg유플러스-데이터센터#평촌LG유플러스 데이터센터":"평촌LG유플러스 데이터센터, LGU+, 2022엘지 유플러스가 경기 안양시 평촌에 구축하는 데이터센터는 축구장 6개를 합친 규모로, 약 10만대 이상의 서버를 운영할 수 있습니다. 2023년 3분기 준공을 목표로 하고 있습니다.","네이버-데이터센터-각-세종#네이버 데이터센터 각 세종":"각 세종, 네이버 클라우드, 2022네이버의 현재 2개의 데이터센터를 갖고 있습니다. 첫 번째는 춘천에 있는 각 춘천이고, 두 번째는 세종에 있는 각 세종입니다.\n이 중 각 세종은 2023년 3 분기 실가동을 목표로 구축하고 있는 하이퍼스케일 데이터센터입니다. 각 세종의 대지 규모는 각 춘천의 6배 규모인 29만 3697m2입니다.\n수전 용량은 각 춘천의 6.7배인 270MW입니다. 또한, 각 세종은 약 60만 유닛 이상의 서버를 수용할 수 있습니다.2","마무리#마무리":"한국은 하이퍼스케일 데이터센터가 두 군데뿐이지만, 앞으로 하이퍼스케일 데이터센터에 대한 수요가 점점 증가함에 따라 더 많은 수의 하이퍼스케일 데이터센터가 구축, 운영될 것으로 보입니다.\n현재는 IDC, 엔터프라이즈 데이터센터와 같은 용어를 흔하게 접할 수 있는데, 앞으로 하이퍼스케일 데이터센터가 많이 구축되면 다른 용어들처럼 일상생활에서 하이퍼스케일 데이터센터라는 용어를 자주 접하게 되리라는 생각이 듭니다.\n이 글이 하이퍼스케일 데이터센터의 기준과 국내외 하이퍼스케일 데이터센터에 대해서 파악하는 데 도움이 되었길 바랍니다.","주석#주석":"1: What is a hyperscale data center?\n2: 네이버클라우드 '각 세종', 오는 3분기 가동, BIKorea","참고#참고":"“전 세계 하이퍼스케일 데이터센터 700개··· 5년 마다 2배 증가”\nTaking another look at the progression of Korea’s first largest hyperscale data center!\nLG유플러스, 평촌에 ‘축구장 6개 규모’ IDC 짓는다"}},"/disaster_recovery/intro":{"title":"아직은 음차가 익숙한 AI 관련 용어들","data":{"":""}},"/":{"title":"Index","data":{"":"IT 관련 정보를 수집하고 정리해서 기록합니다."}},"/logs":{"title":"로그","data":{"":""}},"/network/GSLB":{"title":"GSLB","data":{"":""}},"/technical_writing/ai":{"title":"아직은 음차가 익숙한 AI 관련 용어들","data":{"":"AI 관련 분야는 아직 대중이 AI 용어에 생소한 이유로 많은 용어들을 음차해서 사용하고 있습니다.\n이 용어들을 한글화하려는 시도도 있지만 아직까지는 음차가 더 익숙하네요\n한글화된 용어들이 한국의 IT 업계에 정착하려면 아직은 시간이 더 필요할 것 같습니다.","한글화-용어도-함께-쓰이는-경우#한글화 용어도 함께 쓰이는 경우":"아래는 AI 관련 용어들 중 한글화 용어도 함께 쓰이는 용어들입니다.영문, 음차, 한글화\nfine tunning, 파인 튜닝, 미세 조정\nmachine learning, 머신 러닝, 기계 학습\nA.I(Artificial intelligence), A.I. 또는 에아아이, 인공 지능","음차만-쓰이는-경우#음차만 쓰이는 경우":"아래 용어는 한글화 단어는 없고, 음차만 쓰이는 용어입니다.영문, 음차, 한글화\ndeep learning, 딥 러닝, X\ntransformer, 트랜스포머, X"}},"/technical_writing/loanword":{"title":"이상하고 어색한 외래어 표기","data":{"":"IT와 관련된 용어들은 대부분 영어로 되어있습니다. 그래서 테크니컬 라이팅을 하다 보면 외래어 표기를 해야 할 경우가 많습니다.\n그럴 때마다 한 번쯤은 고민하게 됩니다. 왜냐하면 국립국어원에서 규정한 외래어 표기가 현실에서 사람들이 사용하는 표기와 다른 경우가 많기 때문입니다.\n\n\n피카츄의 올바른 표기는 피카추, MBC 우리말 나들이이 글에서는 IT와 관련된 용어의 외래어 표기를 정리합니다.\n아울러 국립국어원의 외래어 표기가 현실에 정착한 표기와, 국립국어원의 외래어 표기가 아직은 어색하고 낯설게 느껴지는 표기에 대해서 알아봅니다.\n이 글은 수시로 업데이트됩니다.","익숙한-외래어-표기#익숙한 외래어 표기":"아래는 국립국어원의 표기가 거의 정착한 경우에 해당하는 경우입니다.","ad#A~D":"영문\t틀린 표현\t맞는 표현\tappliaction\t어플리케이션\t애플리케이션\tcontents\t컨텐츠\t콘텐츠\tcomponent\t콤포넌트\t컴포넌트\tcabinet\t캐비넷\t캐비닛\tcollection\t콜렉션\t컬렉션","ez#E~Z":"영문\t틀린 표현\t맞는 표현\tfront\t프론트\t프런트\tmessage\t메세지\t메시지\tnavigator\t네비게이터\t내비게이터\trepository\t리파지토리, 레포지토리\t리포지토리","여전히-어색한-외래어-표기#여전히 어색한 외래어 표기":"아래는 사람들이 국립국어원의 표기를 어색하게 느끼고 잘 쓰지 않는 경우입니다.\n영문\t틀린 표현\t맞는 표현\tsolution\t솔루션\t설루션\tthumbnail\t썸네일\t섬네일\tparallel\t페러렐, 패러랠\t패럴렐\tproxy\t프록시\t프락시","참고#참고":"외래어 표기법 질문 (Repository, Cordon), 국립국어원"}},"/AI_trends/pinecone/create_semantic_search_using_pinecone_and_huggingface":{"title":"파인콘, 허깅페이스로 시맨틱 검색 만들기","data":{"":"","시맨틱-검색이란#시맨틱 검색이란":"시맨틱 검색(semantic search)란 사용자가 검색을 하려는 의도를 파악하고, 문서에 나타난 용어의 문맥을 이해해 사용자가 원하는 것과 더 관련성 높은 결과를 생성하는 것을 말합니다. 한국에서는 2009년에 네이트에서 '시맨틱 검색'이라는 이름으로 검색 서비스에 도입했으며, 2013년에 서비스를 중단했습니다. 그래서 네이트를 사용해 본 사람들이라면 시맨틱 검색이라는 용어에 친숙할 것입니다. 아래는 위키피디아의 시맨틱 검색에 대한 설명입니다.\n시맨틱 검색은 검색 엔진이 쿼리의 전체 의미를 이해하지 않고 쿼리 단어 또는 그 변형의 리터럴 일치를 찾는 어휘 검색과 구별되는 의미 검색을 나타냅니다. 시맨틱 검색은 검색자의 의도와 검색 가능한 데이터 공간에 나타나는 용어의 문맥적 의미를 이해함으로써 검색 정확도를 향상하고 더 관련성 높은 결과를 생성합니다. 시맨틱 검색에서 순위가 ​​좋은 콘텐츠는 자연스러운 음성으로 잘 작성되고, 사용자의 의도에 집중하며, 사용자가 향후 찾을 수 있는 관련 주제를 고려한 것입니다.\n이 글에서는 파인콘의 semantic search를 참고해 시맨틱 검색을 Colab을 활용해 만들어봅니다.","시맨틱-검색-만들기#시맨틱 검색 만들기":"","데이터-전처리#데이터 전처리":"파인콘에서 사용할 수 있는 데이터 세트를 준비하는 과정은 아래와 같은 단계를 거칩니다.\n허깅페이스(Huggingface) 데이터 세트에서 쿼라(Quora) 데이터 세트를 다운로드합니다.\n데이터 세트의 텍스트 콘텐츠를 벡터에 포함합니다.\n파인콘에 추가할 수 있는 구조로 변형합니다.\n\n먼저 쿼라의 데이터 세트를 다운로드합니다.\nfrom datasets import load_dataset\n\ndataset = load_dataset('quora', split='train[240000:320000]')\ndataset\n\n\nWARNING:datasets.builder:Found cached dataset quora (/root/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04)\n\n\n\n\n\nDataset({\nfeatures: ['questions', 'is_duplicate'],\nnum_rows: 80000\n})\n\n데이터 세트에는 쿼라의 자연어 질문 쌍이 400,000개 이상이 포함되어 있습니다. 데이터 세트 일부를 출력해서 데이터 형태를 확인합니다.\ndataset[:5]\n\n{'questions': [{'id': [207550, 351729],\n'text': ['What is the truth of life?', \"What's the evil truth of life?\"]},\n{'id': [33183, 351730],\n'text': ['Which is the best smartphone under 20K in India?',\n'Which is the best smartphone with in 20k in India?']},\n{'id': [351731, 351732],\n'text': ['Steps taken by Canadian government to improve literacy rate?',\n'Can I send homemade herbal hair oil from India to US via postal or private courier services?']},\n{'id': [37799, 94186],\n'text': ['What is a good way to lose 30 pounds in 2 months?',\n'What can I do to lose 30 pounds in 2 months?']},\n{'id': [351733, 351734],\n'text': ['Which of the following most accurately describes the translation of the graph y = (x+3)^2 -2 to the graph of y = (x -2)^2 +2?',\n'How do you graph x + 2y = -2?']}],\n'is_duplicate': [False, True, False, True, False]}\n\nis_duplicated 키를 보면 질문이 중복되는지 여부를 확인할 수 있습니다. 사실, 질문의 중복 여부는 중요하지 않습니다. 텍스트만 필요하기 때문에, 텍스트만 추출해서 questions 리스트에 담습니다.\nquestions = []\n\nfor record in dataset['questions']:\nquestions.extend(record['text'])\n\n# remove duplicates\nquestions = list(set(questions))\nprint('\\n'.join(questions[:5]))\nprint(len(questions))\n\n\nDoes outer space ever stop?\nWhy is Eunuch and Son of Bitch more worried than emperor at ASEAM meeting in Laos?\nHow do I apply for Google's Digital Marketing Course?\nHow is the word 'species' used in a sentence?\nWhat will happen if a 10'x10'x8\" concrete slab was thrown at your whole body at say 43 mph?\n136057\n질문이 준비되었으니 이제 임베딩을 진행하고 업서트(upsert)를 할 수 있는 형태로 만듭니다.","임베딩-및-업서트-형식-구축#임베딩 및 업서트 형식 구축":"임베딩을 생성하기 위해서 MiniLM-L6 문장 변환기 모델을 사용합니다. 이 모델은 꽤 효율적인 시맨틱 유사도 임베딩 모델로 sentence-transformers 라이브러리를 통해서 사용할 수 있습니다. 아래 코드로 모델을 초기화합니다.\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nif device != 'cuda':\nprint(f\"You are using {device}. This is much slower than using \"\n\"a CUDA-enabled GPU. If on Colab you can change this by \"\n\"clicking Runtime > Change runtime type > GPU.\")\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device=device)\nmodel\n\n\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n(2): Normalize()\n)\n\n모델을 프린트했을 때 확인할 수 있는 3 가지 요소에 대해 살펴봅니다.\nmax_seq_length가 256입니다. 싱글 벡터 임베딩에 인코딩 될 수 있는 토큰의 최대 개수가 256개라는 뜻입니다. \b256개를 넘으면 잘리게 됩니다.\nword_embeddings_dimension은 이 모델의 벡터 차원이 384이라는 뜻입니다. 나중에 파인콘 벡터 인덱스를 초기화하는데 이 word_embeddings_dimension 값이 필요합니다.\nNormalize()는 모델이 생성한 모든 벡터가 정규화됐다는 것을 의미합니다. 이제 코사인 유사도를 사용해 유사도를 측정하는 모델도 스칼라곱을 사용할 수 있습니다. 사실, 정규화된 벡터 코사인과 스칼라곱은 동일합니다.\n이제 문장 임베딩을 아래와 같이 생성할 수 있습니다.\n\n\nquery = 'which city is the most populated in the world?'\n\nxq = model.encode(query)\nxq.shape\n\n문장을 임베딩 하면, 384차원으로 임베딩이 됩니다.(word_embeddings_dimension과 같은 값입니다.)\n이제 파인콘에 업서트할 수 있도록 준비합니다.\n_id = '0'\nmetadata = {'text': query}\n\nvectors = [(_id, xq, metadata)]\n파인콘에 데이터를 업서트 할 때는 배치 단위로 진행할 것이므로, 벡터는 (id, embedding, metadata) 튜플을 담은 리스트 형태여야 합니다.","인덱스-생성#인덱스 생성":"이제 인덱스를 설정해 데이터를 저장합니다. 먼저 파인콘 연결을 초기화합니다.\nimport os\nimport pinecone\n\n# get api key from app.pinecone.io\nPINECONE_API_KEY = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n# find your environment next to the api key in pinecone console\nPINECONE_ENV = os.environ.get('PINECONE_ENVIRONMENT') or 'PINECONE_ENVIRONMENT'\n\npinecone.init(\napi_key=PINECONE_API_KEY,\nenvironment=PINECONE_ENV\n)\n\nsemantic-search이라는 이름의 새 인덱스를 생성합니다.\nindex_name = 'semantic-search'\n\n# only create index if it doesn't exist\nif index_name not in pinecone.list_indexes():\npinecone.create_index(\nname=index_name,\ndimension=model.get_sentence_embedding_dimension(),\nmetric='cosine'\n)\n\n# now connect to the index\nindex = pinecone.GRPCIndex(index_name)\n\n파인콘에서 생성된 인덱스를 확인할 수 있습니다. 인덱스 이름은 코드에서 지정한 데로 semantic-search이며, metric은 cosine을 사용하는 것을 확인할 수 있습니다.\n\n\n\n파인콘에 생성된 인덱스지금은 인덱스만 생성하고 데이터를 업서트(upsert)하지 않아서 Total Vectors가 0입니다. 이제 데이터를 업서트합니다.\nfrom tqdm.auto import tqdm\n\nbatch_size = 128\n\nfor i in tqdm(range(0, len(questions), batch_size)):\n# find end of batch\ni_end = min(i+batch_size, len(questions))\n# create IDs batch\nids = [str(x) for x in range(i, i_end)]\n# create metadata batch\nmetadatas = [{'text': text} for text in questions[i:i_end]]\n# create embeddings\nxc = model.encode(questions[i:i_end])\n# create records list for upsert\nrecords = zip(ids, xc, metadatas)\n# upsert to Pinecone\nindex.upsert(vectors=records)\n\n# check number of records in the index\nindex.describe_index_stats()\n\n\n0%|          | 0/1063 [00:00<?, ?it/s]\n\n\n\n\n\n{'dimension': 384,\n'index_fullness': 0.1,\n'namespaces': {'': {'vector_count': 136057}},\n'total_vector_count': 136057}\n이제 파인콘 인덱스 페이지 페이지에서 업서트된 벡터 수를 확인할 수 있습니다.\n\n\n\n벡터를 업서트한 파인콘 인덱스매트릭 탭에서는 시간에 따른 벡터 수 변화도 확인할 수 있습니다.\n\n\n파인콘 인덱스의 벡터 수 변화 그래프","쿼리-만들기#쿼리 만들기":"인덱스가 벡터로 채워졌습니다. 이제 질문을 보내서 시맨틱 검색을 할 수 있습니다. 아래의 예제를 보내 시맨틱 검색을 테스트해 봅니다.\nquery = \"which city has the highest population in the world?\"\n\n# create the query vector\nxq = model.encode(query).tolist()\n\n# now query\nxc = index.query(xq, top_k=5, include_metadata=True)\nxc\n\n\n{'matches': [{'id': '31072',\n'metadata': {'text': 'What country has the biggest population?'},\n'score': 0.7655585,\n'sparse_values': {'indices': [], 'values': []},\n'values': []},\n{'id': '23769',\n'metadata': {'text': 'What is the biggest city?'},\n'score': 0.7271395,\n'sparse_values': {'indices': [], 'values': []},\n'values': []},\n{'id': '65783',\n'metadata': {'text': 'What is the most isolated city in the '\n'world, with over a million metro area '\n'inhabitants?'},\n'score': 0.7020447,\n'sparse_values': {'indices': [], 'values': []},\n'values': []},\n{'id': '104484',\n'metadata': {'text': 'Which is the most beautiful city in '\n'world?'},\n'score': 0.69991666,\n'sparse_values': {'indices': [], 'values': []},\n'values': []},\n{'id': '79997',\n'metadata': {'text': 'Where is the most beautiful city in the '\n'world?'},\n'score': 0.69605494,\n'sparse_values': {'indices': [], 'values': []},\n'values': []}],\n'namespace': ''}\n\n파인콘을 무료로 사용하고 있다면 인덱스는 1개만 저장할 수 있습니다. 아래 코드로 이번 예제에 사용한 인덱스를 삭제합니다.\npinecone.delete_index(index_name)","참고#참고":"네이트에 의미기반 '시맨틱 검색' 뜬다\nWikipeida Semantic search"}}}